{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ec49c-a31e-45c4-a5d6-e18ddbbebbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ecfa43-e1ac-4e52-aa1a-a9cfc37b8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_FOLDER_PATH='C:\\\\Users\\\\renat\\\\Projetos\\\\Harmful Brain Activity Classification'\n",
    "\n",
    "TARGETS = ['seizure_vote','lpd_vote','gpd_vote','lrda_vote','grda_vote','other_vote']\n",
    "\n",
    "CHAINS = {\n",
    "    'LL' : [(\"Fp1\",\"F7\"),(\"F7\",\"T3\"),(\"T3\",\"T5\"),(\"T5\",\"O1\")],\n",
    "    'RL' : [(\"Fp2\",\"F8\"),(\"F8\",\"T4\"),(\"T4\",\"T6\"),(\"T6\",\"O2\")],\n",
    "    'LP' : [(\"Fp1\",\"F3\"),(\"F3\",\"C3\"),(\"C3\",\"P3\"),(\"P3\",\"O1\")],\n",
    "    'RP' : [(\"Fp2\",\"F4\"),(\"F4\",\"C4\"),(\"C4\",\"P4\"),(\"P4\",\"O2\")]\n",
    "}\n",
    "\n",
    "EEG_COL_NAMES=['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz',\n",
    "       'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n",
    "\n",
    "EEG_DDP_COL_NAMES=['Fp1-F7', 'F7-T3', 'T3-T5', 'T5-O1', 'Fp2-F8', 'F8-T4', 'T4-T6', 'T6-O2',\n",
    "               'Fp1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'Fp2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'EKG']\n",
    "\n",
    "K = 1.5 # parâmetro para detecção de outliers (e.g limInf = q1 - k*IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a9090-6ee0-45a5-9a55-6e2c2211ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(row,metadata,window=True):\n",
    "    \n",
    "    eeg_id = str(metadata.loc[row,'eeg_id'])\n",
    "    eeg_offset = int(metadata.loc[row,'eeg_label_offset_seconds'])\n",
    "    \n",
    "    spect_id = str(metadata.loc[row,'spectrogram_id'])\n",
    "    spect_offset = int(metadata.loc[row,'spectrogram_label_offset_seconds'])\n",
    "    \n",
    "    eeg = pd.read_parquet(MAIN_FOLDER_PATH+f'\\\\train_eegs\\\\{eeg_id}.parquet')\n",
    "    spect = pd.read_parquet(MAIN_FOLDER_PATH+f'\\\\train_spectrograms\\\\{spect_id}.parquet')\n",
    "    \n",
    "    # fill nans with the mean\n",
    "    \n",
    "    eeg=eeg.fillna(eeg.mean())\n",
    "    spect=spect.fillna(spect.mean())\n",
    "    \n",
    "    if window:        \n",
    "        length=10\n",
    "        eeg = eeg.iloc[(eeg_offset+20)*200:(eeg_offset+30)*200]\n",
    "        spect = spect.loc[(spect.time>=spect_offset+295)&(spect.time<spect_offset+305)]\n",
    "        \n",
    "    else:\n",
    "        length=50\n",
    "        eeg = eeg.iloc[eeg_offset*200:(eeg_offset+50)*200]\n",
    "        spect = spect.loc[(spect.time>=spect_offset)&(spect.time<spect_offset+600)]\n",
    "        \n",
    "    label = metadata.loc[row,TARGETS].values\n",
    "        \n",
    "    return eeg, spect, label\n",
    "\n",
    "def load_data(data,window=True):\n",
    "    \n",
    "    eeg_tensor=[]\n",
    "    sp_tensor=[]\n",
    "    labels=[]\n",
    "    \n",
    "    for row_index in range(data.shape[0]):\n",
    "\n",
    "        try:\n",
    "            eeg, spect, label = get_sample(row_index,metadata=data,window=window)       \n",
    "        except:\n",
    "            print(f'There was a problem with row {row_index}!')\n",
    "        else:\n",
    "            eeg_tensor.append(eeg.values)\n",
    "            sp_tensor.append(spect.iloc[:,1:].values)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.stack(eeg_tensor), np.stack(sp_tensor), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c393d9-3bbf-41aa-b510-5a2b0311dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna um mapa indicando quais colunas, em cada amostra, possuem valores aberrantes \n",
    "\n",
    "def tell_liers(eegs, return_lims=False):\n",
    "\n",
    "    liers=pd.DataFrame({col:[0]*eegs.shape[0] for col in EEG_COL_NAMES})\n",
    "    \n",
    "    limites_dic=defaultdict(list)\n",
    "    \n",
    "    for col in range(eegs.shape[2]):\n",
    "\n",
    "        limites = {'lim_inf':[],'lim_sup':[]}\n",
    "\n",
    "        for index, sample in enumerate(eegs):\n",
    "\n",
    "            arr=sample[:,col]\n",
    "\n",
    "            #if np.std(arr)==0: liers.loc[index,EEG_COL_NAMES[col]]=1\n",
    "\n",
    "            q1, q2, q3 = np.percentile(arr, [25, 50, 75])\n",
    "            iqr = q3 - q1\n",
    "\n",
    "            lim_sup = q3 + K * iqr\n",
    "            lim_inf = q1 - K * iqr\n",
    "\n",
    "            limites['lim_inf'].append(lim_inf)\n",
    "            limites['lim_sup'].append(lim_sup)\n",
    "\n",
    "        # calcular os LIM_INF e LIM_SUP dos pools de lim_infs e lim_sups       \n",
    "\n",
    "        pool_infs = np.array(limites['lim_inf'])\n",
    "        pool_sups = np.array(limites['lim_sup'])\n",
    "        \n",
    "        limites_dic[col]+=[pool_infs,pool_sups]\n",
    "        \n",
    "        q_infs = np.percentile(pool_infs, [25, 50, 75])\n",
    "        q_sups = np.percentile(pool_sups, [25, 50, 75])\n",
    "\n",
    "        iqr_infs = q_infs[2] - q_infs[0] # IQR do conjunto de limites inferiores\n",
    "        iqr_sups = q_sups[2] - q_sups[0] # IQR do conjunto de limites superiores\n",
    "\n",
    "        LIM_INF = q_infs[0] - K * iqr_infs\n",
    "        LIM_SUP = q_sups[2] + K * iqr_sups\n",
    "\n",
    "        # agora, considero outlier toda amostra cujo lim_inf seja menor que LIM_INF e toda amostra cujo lim_sup seja maior que LIM_SUP\n",
    "\n",
    "        for index, lim in enumerate(zip(limites['lim_inf'],limites['lim_sup'])):\n",
    "            if lim[0]<LIM_INF or lim[1]>LIM_SUP:\n",
    "                liers.loc[index,EEG_COL_NAMES[col]]=1\n",
    "                \n",
    "    if return_lims==True:\n",
    "        \n",
    "        return liers, limites_dic\n",
    "    \n",
    "    elif return_lims==False:\n",
    "        \n",
    "        return liers\n",
    "    \n",
    "\n",
    "def big_liers(liers, threshold=10):\n",
    "    \n",
    "    # retorna um dataframe contendo as amostras que devem ser removidas\n",
    "    \n",
    "    big_liers=[]\n",
    "    \n",
    "    for index, row in liers.iterrows():\n",
    "        \n",
    "        if row.sum()>threshold:\n",
    "            big_liers.append(index)\n",
    "            \n",
    "    print(f'Número de big liers: {len(big_liers)}')        \n",
    "    print(f'Proporção de outliers: {len(big_liers)/liers.shape[0]}')\n",
    "    \n",
    "    return big_liers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ccb24-f850-4fac-86ff-917ee04daa74",
   "metadata": {},
   "source": [
    "## Load metadata, select non-overlaping samples and transform labels into probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5dfcc-e7f5-400c-8abd-8e640e4ef0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(MAIN_FOLDER_PATH+'\\\\train.csv')\n",
    "\n",
    "df=train.groupby('eeg_id')[['spectrogram_id', 'eeg_label_offset_seconds','spectrogram_label_offset_seconds', 'patient_id']].agg({\n",
    "    'spectrogram_id':'first', 'eeg_label_offset_seconds':'min', 'spectrogram_label_offset_seconds':'min', 'patient_id':'first'})\n",
    "\n",
    "\n",
    "# normalizing labels\n",
    "\n",
    "tmp = train.groupby('eeg_id')[TARGETS].agg('sum')\n",
    "for t in TARGETS:\n",
    "    df[t] = tmp[t].values\n",
    "    \n",
    "y_data = df[TARGETS].values\n",
    "y_data = y_data / y_data.sum(axis=1,keepdims=True)\n",
    "df[TARGETS] = y_data\n",
    "\n",
    "tmp = train.groupby('eeg_id')[['expert_consensus']].agg('first')\n",
    "df['target'] = tmp\n",
    "\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3efd962-acbb-4eba-bbda-7b3d5d63288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "sns.histplot(data=train.expert_consensus, ax=ax[0])\n",
    "ax[0].set_title(\"'train' target distribution\")\n",
    "\n",
    "sns.histplot(data=df.target, ax=ax[1])\n",
    "ax[1].set_title(\"'df' target distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc6a40-fad8-4315-9324-f819e86f61b7",
   "metadata": {},
   "source": [
    "## Load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98026f12-1b90-4784-9cec-8cd592a95ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grouped(df, column='target',window=True):\n",
    "    \n",
    "    output = defaultdict(list)\n",
    "    \n",
    "    groups=df.groupby(column)\n",
    "    \n",
    "    for group_name, group_data in groups:\n",
    "        \n",
    "        print(f'Loading group {group_name}.')\n",
    "        \n",
    "        eegs, spects, labels = load_data(group_data.reset_index().drop('index', axis=1),window=window)\n",
    "        \n",
    "        output[group_name]+=[eegs, spects, labels]\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175b23d-54bf-4d66-8a0a-cf607b502609",
   "metadata": {},
   "source": [
    "## Load data from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c5060-c123-4278-8e9d-7eca67f752b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('data_dict.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e758e-18a2-4fb3-baca-485037d23016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfd07b-9651-426b-b5b7-3fb8450ef5cf",
   "metadata": {},
   "source": [
    "## Split data into train set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e6e40-369d-4bfd-824b-68bc9578fe04",
   "metadata": {},
   "source": [
    "Estou fazendo o split antes da remoção dos outliers. Isso significa que pode haver outliers no conjunto de teste. Como não sabemos se os dados de teste da competição possuem colunas absurdas, acredito que vale a pena tentar treinar o modelo dos dois jeitos: removendo os outliers antes e depois do split. Por ora seguirei com a segunda opção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a059855-749d-4a89-b874-d5dfa917eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86bd2e-0dfc-4ad6-9c40-f2f5650fb8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for label, dados in data.items():\n",
    "    \n",
    "    eegs = dados[0]\n",
    "    specs = dados[1]\n",
    "    labels = dados[2]\n",
    "    \n",
    "    idx = [i for i in range(eegs.shape[0])]\n",
    "    random.shuffle(idx)\n",
    "    \n",
    "    train_idx = idx[0:int(train_ratio*len(idx))]\n",
    "    test_idx = idx[int(train_ratio*len(idx)):]\n",
    "    \n",
    "    train_data[label] = [eegs[train_idx], specs[train_idx], labels[train_idx]]\n",
    "    test_data[label] = [eegs[test_idx], specs[test_idx], labels[test_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99227d6-ef2a-451f-bb9d-3004fb4a72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(data, ax):\n",
    "    \n",
    "    labels = []\n",
    "    freqs = []\n",
    "    \n",
    "    for label, dados in data.items():\n",
    "        \n",
    "        labels.append(label)\n",
    "        freqs.append(dados[0].shape[0])\n",
    "    \n",
    "    ser = []\n",
    "    \n",
    "    for label, freq in zip(labels,freqs):\n",
    "        ser+=[label]*freq\n",
    "    \n",
    "    ser = pd.Series(ser)\n",
    "    \n",
    "    sns.histplot(ser, ax=ax, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cb6c7-a533-4490-a9f1-300b95e1defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,figsize=(20,20))\n",
    "\n",
    "titles=['data','train data', 'test data']\n",
    "\n",
    "for i, dados in enumerate([data, train_data, test_data]):\n",
    "    \n",
    "    plot_target_distribution(dados,ax[i])\n",
    "    ax[i].set_title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e42cbd-a15e-4046-b181-31ed5cebc1bc",
   "metadata": {},
   "source": [
    "## Out, liers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7c91d-d9fd-4094-a4f8-5e3e4f5d2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fea7e-58a3-4420-a8fd-60a30a079f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "liers=tell_liers(train_data['Seizure'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614042bb-438e-49c0-906d-d78f70b09457",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigliers=big_liers(liers, threshold=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae141a07-3e25-4344-9e8d-402efeec1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "liers=tell_liers(train_data['Other'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef071171-e687-45b8-9173-b11843386d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigliers=big_liers(liers, threshold=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2bafe3-eb30-40c4-8cbc-ba75cc31ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data,threshold=THRESHOLD):\n",
    "    \n",
    "    output={}\n",
    "    \n",
    "    for label, dados in data.items():\n",
    "        \n",
    "        print(label)\n",
    "        \n",
    "        eegs = dados[0]\n",
    "        specs = dados[1]\n",
    "        labels = dados[2]\n",
    "        \n",
    "        liers = tell_liers(eegs)\n",
    "        idx_to_drop = big_liers(liers, threshold=threshold)\n",
    "        \n",
    "        output[label] = [np.delete(eegs, idx_to_drop, axis=0),np.delete(specs, idx_to_drop, axis=0),np.delete(labels, idx_to_drop, axis=0)]\n",
    "        \n",
    "        print('_________________________________________________________________________')\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db20b0-09a7-4a3a-94af-e06a055f0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = remove_outliers(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38289ab9-b515-4dc5-aa0b-503aa4997533",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_target_distribution(train_data,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113a4ac-b198-4ff9-9165-70220d7b5b72",
   "metadata": {},
   "source": [
    "## Prune \"Other\" in train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd93835-a403-42c8-815f-b170113d9311",
   "metadata": {},
   "source": [
    "Amostras classificadas como \"Other\" são muito mais frequentes. Como não queremos que o modelo fique especialista em prever \"Other\" e esqueça de suas outras responsabilidades, irei remover amostras dessa classe até que os dados estejam mais balanceados.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ecec9-ea5e-4475-9405-72e6f09738c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular a média das quantidades de amostras de cada classe, exceto Other. \n",
    "\n",
    "lengths=[]\n",
    "\n",
    "for label, dados in train_data.items():\n",
    "    \n",
    "    print(label)\n",
    "    \n",
    "    eegs=dados[0]\n",
    "    specs=dados[1]\n",
    "    labels=dados[2]\n",
    "    \n",
    "    length=eegs.shape[0]\n",
    "    \n",
    "    print(f'{length} samples')\n",
    "    print('_________________________________________________________________________')\n",
    "    \n",
    "    if label != 'Other':\n",
    "        lengths.append(length)\n",
    "    \n",
    "media_train = sum(lengths)/len(lengths)\n",
    "\n",
    "print(f'Media (tirando \"Other\"): {media_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f111666-58be-4da3-bf23-a44b9cf0f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular a média das quantidades de amostras de cada classe, exceto Other. \n",
    "\n",
    "lengths=[]\n",
    "\n",
    "for label, dados in test_data.items():\n",
    "    \n",
    "    print(label)\n",
    "    \n",
    "    eegs=dados[0]\n",
    "    specs=dados[1]\n",
    "    labels=dados[2]\n",
    "    \n",
    "    length=eegs.shape[0]\n",
    "    \n",
    "    print(f'{length} samples')\n",
    "    print('_________________________________________________________________________')\n",
    "    \n",
    "    if label != 'Other':\n",
    "        lengths.append(length)\n",
    "    \n",
    "media_test = sum(lengths)/len(lengths)\n",
    "\n",
    "print(f'Media (tirando \"Other\"): {media_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0332aad-8918-4433-9166-828c8c805d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Other']=[train_data['Other'][0][0:int(media_train)],train_data['Other'][1][0:int(media_train)],train_data['Other'][2][0:int(media_train)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967cfdd3-4cf2-4084-ab50-0717de996d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Other']=[test_data['Other'][0][0:int(media_test)],test_data['Other'][1][0:int(media_test)],test_data['Other'][2][0:int(media_test)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b37d07-8f4a-449d-9702-1e29505f023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "\n",
    "plot_target_distribution(train_data, ax=ax[0])\n",
    "ax[0].set_title('Pruned train data')\n",
    "\n",
    "plot_target_distribution(test_data, ax=ax[1])\n",
    "ax[1].set_title('Pruned test data')\n",
    "\n",
    "fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0, hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67041d-a42e-46ca-a72e-be125a8a7bbd",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed8bf6-123d-4dde-9d2a-eb11c557190d",
   "metadata": {},
   "source": [
    "Os dados ainda estão desbalanceados. Sugiro um aumento de amostras da classe LRDA, por exemplo. \n",
    "No entanto, vou passar esta etapa por enquanto. Depois testamos diferentes abordagens de data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5dbc9-298e-4ffd-9018-41bb8677c6f1",
   "metadata": {},
   "source": [
    "## Handle columns with std=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f157583-a6f0-4fbc-b466-45e8fccad4a4",
   "metadata": {},
   "source": [
    "Há colunas com std=0. Aceito sugestões de como tratar esses casos. Pensei em clusterizar as amostras por similaridade entre os vetores label normalizados e selecionar uma amostra representativa do cluster para \"doar\" cópias de suas colunas para outras amostras do mesmo cluster. Podemos inclusive aplicar uma softmax com temperatura elevada nas labels antes de clusterizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ed352-f1ae-4cef-975b-dd464b004c53",
   "metadata": {},
   "source": [
    "## Concatenate classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c78ef2-4cbe-4714-82f2-7656497361bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eegs = np.concatenate([dados[0] for label, dados in train_data.items()])\n",
    "train_specs = np.concatenate([dados[1] for label, dados in train_data.items()])\n",
    "train_labels = np.concatenate([dados[2] for label, dados in train_data.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97960a-e2d6-452e-8277-6eb65cee857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eegs = np.concatenate([dados[0] for label, dados in test_data.items()])\n",
    "test_specs = np.concatenate([dados[1] for label, dados in test_data.items()])\n",
    "test_labels = np.concatenate([dados[2] for label, dados in test_data.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b9e2d-89d1-4582-8a31-b06153351f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eegs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409976b-1bc2-4245-acff-27d7ae5704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eegs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14427ee1-bcc1-45cf-8479-c5d24efd7e02",
   "metadata": {},
   "source": [
    "## Denoise EEGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2466d2-99bc-41e0-993a-92c3754fe469",
   "metadata": {},
   "source": [
    "Não implementei esta etapa, mas é algo que vale a pena considerarmos. \n",
    "\n",
    "Sugestão abaixo. Peguei desse link: https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg?scriptVersionId=159854820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afead4-0dd5-411e-a6b9-b632ca9b0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENOISE FUNCTION\n",
    "# testar diferentes valores para \"wavelet\"\n",
    "\n",
    "#import pywt\n",
    "\n",
    "def maddest(d, axis=None):\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
    "\n",
    "def denoise(x, wavelet='haar', level=1):    \n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * maddest(coeff[-level])\n",
    "\n",
    "    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "\n",
    "    ret=pywt.waverec(coeff, wavelet, mode='per')\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad8261a-cef1-4f11-86e1-9bf474b338c4",
   "metadata": {},
   "source": [
    "## Convert EEGs into Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904a2cb-72f7-4b9c-8f1c-1bc19c0e447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(data, columns=EEG_COL_NAMES):   \n",
    "    df=pd.DataFrame(data=data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf68a0-da98-44af-82fc-666e20dfeac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = to_df(train_eegs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4dfb4b-777c-4a95-8326-2dff84bd043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a665a8-ea2f-443f-b26b-026a1386d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPERSEG = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712557c5-e571-4df2-b753-003c5a9bcd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec(data, nperseg=NPERSEG):\n",
    "    return sp.signal.spectrogram(data,fs=200,nperseg=nperseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b57d3a-72c7-4477-845d-c1604887f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t , s = spec(df_sample['Fp1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feedfe4-9b4e-4ca4-b849-be81a9044e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3dcfc-3bf6-4418-8c59-11aff1dfdc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edbfe5-2198-4e98-b3eb-e226cbbe2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.7*201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f51b3-5d2a-432c-8216-a41a240d3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "f[1:141]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa5b4c-fd84-44f7-9485-883b0b9c91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAINS = {\n",
    "    'LL' : [(\"Fp1\",\"F7\"),(\"F7\",\"T3\"),(\"T3\",\"T5\"),(\"T5\",\"O1\")],\n",
    "    'RL' : [(\"Fp2\",\"F8\"),(\"F8\",\"T4\"),(\"T4\",\"T6\"),(\"T6\",\"O2\")],\n",
    "    'LP' : [(\"Fp1\",\"F3\"),(\"F3\",\"C3\"),(\"C3\",\"P3\"),(\"P3\",\"O1\")],\n",
    "    'RP' : [(\"Fp2\",\"F4\"),(\"F4\",\"C4\"),(\"C4\",\"P4\"),(\"P4\",\"O2\")]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35302d3c-5223-422f-88e7-8cef138ee39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_spec(data):\n",
    "    \n",
    "    freqs, t, s = spec(data['Fp1'])\n",
    "    n_rows=t.shape[0]\n",
    "    f_lim = int(0.7*freqs.shape[0])+1\n",
    "    output = []\n",
    "    \n",
    "    for region in CHAINS.keys():\n",
    "        \n",
    "        magic_spec = pd.DataFrame({region+str(f):[0]*n_rows for f in freqs[1:f_lim]})\n",
    "        \n",
    "        for pair in CHAINS[region]:\n",
    "            \n",
    "            f, t, s1 = spec(data[pair[0]])\n",
    "            f, t, s2 = spec(data[pair[1]])\n",
    "            \n",
    "            magic_spec = magic_spec.add(pd.DataFrame(data=s1[1:f_lim].T-s2[1:f_lim].T, columns=[region+str(freq) for freq in f[1:f_lim]]))\n",
    "            \n",
    "        output.append(magic_spec/4)\n",
    "        \n",
    "    return pd.concat(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c51051-3ad3-4ab0-bf28-bfc3205cbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_spec(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268703d-3692-4ce0-bb4f-b9394180a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eegs_to_spec(eegs):\n",
    "    \n",
    "    output=[]\n",
    "    \n",
    "    for eeg in eegs:\n",
    "        \n",
    "        spec = magic_spec(to_df(eeg)).values\n",
    "        \n",
    "        output.append(spec)\n",
    "        \n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f88576-d307-4d72-a196-5f2126f76270",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eegs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33588ac4-9bc0-451b-b7d3-2f78d3e874a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tr_train_eegs = eegs_to_spec(train_eegs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35629ea3-df21-48eb-bbba-a793ea0d6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_train_eegs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057f6f4-4307-4052-9ddd-2b310553d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eegs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ac0b9-9753-4922-a60f-213266b2b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tr_test_eegs = eegs_to_spec(test_eegs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80cd579-b6bd-4c41-8a56-9791f382b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_test_eegs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1608c6c-ea58-4991-bc7b-3cd7ed153bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237e092-4397-4f5a-90b2-0c51e05b3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_specs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8d048-b886-45cf-858b-d93c3a909d4b",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Temos agora:\n",
    "\n",
    "- tr_train_eegs\n",
    "- train_specs\n",
    "- train_labels\n",
    "- tr_test_eegs\n",
    "- test_specs\n",
    "- test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca1d12-462f-43f5-b6b5-d588a35ea877",
   "metadata": {},
   "source": [
    "## Merge EEG Specs with Kaggle Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a363c8-6ad5-4171-b4c1-37592dab4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_data(tr_eegs, specs):\n",
    "    \n",
    "    if tr_eegs.shape[1]==specs.shape[1]: # verifica se os dois arrays têm o mesmo número de linhas\n",
    "        \n",
    "        output=[]\n",
    "        \n",
    "        for tr_eeg, spec in zip(tr_eegs, specs):\n",
    "            \n",
    "            output.append(np.concatenate((tr_eeg, spec), axis=1))\n",
    "        \n",
    "        return np.stack(output)\n",
    "                          \n",
    "    else:\n",
    "        print('É preciso implentar o preenchimento de um dos arrays até que ambos tenham o mesmo shape.')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c629288-f6a2-43ec-87c6-01c10125ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train = assemble_data(tr_train_eegs, train_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6208362-838a-4e5e-ab0e-9dfb690e2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b0c10-011b-4483-b3d4-2127b20dad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f25d0-4624-487b-89bd-ca0dcd604a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be274c4-6a5c-4732-81e0-6995526c66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = assemble_data(tr_test_eegs, test_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19891b2-d1b2-4194-91a2-c6bf28d30864",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c582280-b743-4129-bb96-004fa91135cb",
   "metadata": {},
   "source": [
    "## Standardization (mean=0, std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59061c2b-43a1-47d4-83a7-c05a00c9608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(tensor):\n",
    "    \n",
    "    normalized_tensor=[]\n",
    "    \n",
    "    for data in tensor:\n",
    "        # Calculate mean and standard deviation for each column\n",
    "        mean = np.mean(data, axis=0)\n",
    "        std_dev = np.std(data, axis=0)\n",
    "        if 0 in std_dev: std_dev+=0.0000001\n",
    "        # Center the data by subtracting the mean\n",
    "        centered_data = data - mean\n",
    "\n",
    "        # Scale the data by dividing by the standard deviation\n",
    "        normalized_data = centered_data / std_dev\n",
    "\n",
    "        normalized_tensor.append(normalized_data)\n",
    "        \n",
    "    return np.stack(normalized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0657a1-de8b-44ef-90af-bcf68d33f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize_data(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b45fb-c49c-4d78-b11a-048f22da5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = normalize_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f774396-03a5-4ae5-893c-ec5dc71158a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data={'X_train':X_train,'y_train':y_train,'X_test':X_test,'y_test':y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735d8b2-01c5-4d22-9ec7-18697ea2852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the dictionary\n",
    "file_path = 'tr_data.pkl'\n",
    "\n",
    "# Open the file in binary write mode and save the dictionary using pickle\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(tr_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb8356-3dc0-4289-b070-966ebea004ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
